/* ==========================================
## dvr.diderot: basic direct volume rendering of scalar field

Note that this example is heavily based on the [`mip`](../mip)
example, especially for the basic set-up of camera, rays, and strands;
some of the variables and code are better documented there.

This program needs a dataset `vol.nrrd` to render; [`fs3d-scl`](../fs3d) is one
way to generate this.  Some examples:
* Sphere:  
   `../fs3d/fs3d-scl -which 4 -width 3 | unu save -f nrrd -o vol.nrrd`
* XYZ axes indicator:  
   `../fs3d/fs3d-scl -which 12 -width 3.5 -sz0 93 -sz1 94 -sz2 95 | unu crop -min 30 30 30 -max M M M -o vol.nrrd`

The program also uses a univariate colormap `cmap.nrrd` (not for any
important purpose, just to show how it could be used). This can be created
via:

	cp ../cmap/isobow.nrrd cmap.nrrd

The program can be compiled, assuming the directions at
https://github.com/Diderot-Language/examples, with:

	../../vis12/bin/diderotc --exec dvr.diderot

Some example invocations, organized according to dataset examples above
(still in-progress...):
* Sphere:  
   `./dvr ...`
* XYZ axes indicator:  
   `./dvr -camEye 3.1 4.0 2.1 \
      -camAt 0.2 0.0 0.4 \
      -camUp 0 0 1 \
      -camNear -0.7 -camFar 0.5 \
      -camFOV 20 -iresU 560 -iresV 480 \
      -thick 0.03 -rayStep 0.005 -maxAlpha 1 -transp0 0.01 \
      -isoval 1 -litdir -1 -2 -1 -mcnear 1.2 1.0 0.8 -mcfar 0.5 0.7 0.9`
========================================== */

input image(3)[] vol ("volume dataset to render") = image("vol.nrrd");
/* see ../mip/mip.diderot for everything about camera set-up */
input vec3 camEye ("camera look-from point") = [6, 9, 2];
input vec3 camAt ("camera look-at point") = [0, 0, 0];
input vec3 camUp ("camera pseudo-up vector") = [0, 0, 1];
input real camNear ("relative to look-at point, distance to near clipping plane (where rays start from)") = -3;
input real camFar ("relative to look-at point, distance to far clipping plane") = 3;
input real camFOV ("field-of-view angle (in degrees) subtended vertically by view plane") = 15;
input bool camOrtho ("whether to use orthographic, instead of perspective, projection") = false;
input int iresU ("# samples on horizontal axis of image") = 640;
input int iresV ("# samples on vertical axis of image") = 480;
input real rayStep ("inter-sample distance along view direction") = 0.1;

input real refStep ("reference (unit) step length, for normalizing opacities") = 0.1;
input real transp0 ("transparency close enough to 0 to terminate ray") = 0.01;
input real isoval ("isovalue at which to render soft isosurface") = 0;
input real thick ("approximate thickness (in world-space) of soft isosurface") = 0.1;
input real maxAlpha ("maximum opacity on rendered surface") = 1.0;
input real phongKa ("Blinn-Phong ambient component") = 0.2;
input real phongKd ("Blinn-Phong diffuse component") = 0.7;
input real phongKs ("Blinn-Phong specular component") = 0.2;
input real phongSp ("Blinn-Phong specularity exponent") = 60;
input vec3 litdir ("direction (non-normalized) towards light source, in (U,V,N) view-space") = [0, 0, -4];
input vec3 mcnear ("material color at near clipping plane (for depth cuing)") = [1,1,1];
input vec3 mcfar ("material color at far clipping plane") = [1,1,1];
/* this fog has no practical value except as a way to get an indication
   of the extent and shape of the volume data domain */
input vec4 fog ("fog RGBA inside volume data domain") = [1,1,1,0];
input image(1)[3] cmap ("univariate colormap") = image("cmap.nrrd");
input real cmin ("value mapped to min end of colormap") = 0;
input real cmax ("value mapped to max end of colormap. By default cmin==cmax==0, which disables this colormapping") = 0;

/* Convolve volume data with one of various possible kernels;
   see ../mip/mip.diderot for more info */
//field#0(3)[] V = tent ⊛ vol;
//field#1(3)[] V = ctmr ⊛ vol;
field#2(3)[] V = bspln3 ⊛ vol;
//field#4(3)[] V = c4hexic ⊛ vol;

/* create a field to render from the original volume data field */
field#1(3)[] F = V - isoval;

/* create a 1-D field around the colormap */
field#0(1)[3] CM = tent ⊛ clamp(cmap);

// (boilerplate) computing ray parameters and view-space basis
vec3 camN = normalize(camAt - camEye);  // N: away from eye
vec3 camU = normalize(camN × camUp);    // U: right
vec3 camV = camN × camU;                // V: down (right-handed frame)
real camDist = |camAt - camEye|;
real camVmax = tan(camFOV*π/360)*camDist;
real camUmax = camVmax*iresU/iresV;
real camNearVsp = camNear + camDist; // near clip, view space
real camFarVsp = camFar + camDist;   // far clip, view space
// convert light directions from view-space to world-space
vec3 litwsp = transpose([camU,camV,camN])•normalize(litdir);

/* 2-D opacity function, after Levoy. The 1.4 is a trick to ensure
  that there is a segment of positions receiving maximum opacity. */
function real alpha(real v, real g) = maxAlpha*clamp(0, 1, 1.4 - |v|/(g*thick));

// how to render ray through (rayU,rayV) on view plane
strand raycast(int ui, int vi /* real rayU, real rayV */) {
   // cell-centered sampling of view plane (intersects look-at)
   real rayU = lerp(-camUmax, camUmax, -0.5, ui, iresU-0.5);
   real rayV = lerp(-camVmax, camVmax, -0.5, vi, iresV-0.5);
   // creation of per-strand ray state based on ../mip/mip.diderot
   real rayN = camNearVsp;
   vec3 UV = rayU*camU + rayV*camV;
   vec3 rayOrig = camEye + (UV if camOrtho else [0,0,0]);
   vec3 rayVec = camN + ([0,0,0] if camOrtho else UV/camDist);
   /* alphaFix is used for opacity correction, so attenuation
   happens in world-space rather than ray sample index space.
   The actual distance between samples on this ray is |rayVec|*rayStep */
   real alphaFix = |rayVec|*rayStep/refStep;
   vec3 eyeDir = -normalize(rayVec);
   // output for this ray
   output vec4 rgba = [0,0,0,0];
   // state for this ray is current color ...
   vec3 rgb = [0,0,0];
   // ... and current tranparency
   real transp = 1;
   /* example of turning on debuging for one strand (pixel);
      can can then have "if (verb) { print(...); }"
   bool verb = 369==ui && 242==vi; */

   update {
      rayN += rayStep;          // increment ray position
      if (rayN > camFarVsp) {   // ray passed the far clipping plane
         stabilize;
      }
      vec3 pos = rayOrig + rayN*rayVec;  // pos is ray sample position
      if (!inside(pos,F)) {              // If not inside field domain,
         continue;                       // then move onto next iteration
      }
      // compute fog contribution
      real aa = fog[3];
      if (aa > 0) {
         aa = 1 - (1 - aa)^alphaFix;
         rgb += transp*aa*[fog[0],fog[1],fog[2]];
         transp *= 1 - aa;
      }
      // compute data contribution
      real val = F(pos);
      vec3 grad = -∇F(pos);
      aa = alpha(val, |grad|);
      if (aa == 0) {
         continue;
      }
      aa = 1 - (1 - aa)^alphaFix;
      // Note: not standard Phong diffuse (no dark hemisphere)
      real dcomp = lerp(0, 1, -1, normalize(grad)•litwsp, 1)^2;
      // Slightly faster as this conditional expression
      real scomp = max(0,normalize(grad)•normalize(eyeDir+litwsp))^phongSp
                   if phongKs != 0 else 0.0;
      // simple depth-cueing
      vec3 dcol = lerp(mcnear, mcfar, camNearVsp, rayN, camFarVsp);
      // lots of things could be basis for material color; this
      // is contrived to show off some univariate mapping
      vec3 mcol = [1,1,1];
      if (cmin != cmax) {
         mcol = CM(lerp(0, 1, cmin, pos[2], cmax));
      }
      // light color is currently [1,1,1]
      rgb += transp*aa*((phongKa + phongKd*dcomp)*modulate(dcol,mcol)
                        + phongKs*scomp*[1,1,1]);
      transp *= 1 - aa;
      if (transp < transp0) { // early ray termination
         transp = 0;
         stabilize;
      }
   }
   stabilize {
      if (transp < 1) {  // un-pre-multiply opacities
         real aa = 1-transp;
         rgba = [rgb[0]/aa, rgb[1]/aa, rgb[2]/aa, aa];
      }
   }
}

initially [ raycast(ui, vi)
            | vi in 0..iresV-1,   // slower
              ui in 0..iresU-1 ]; // faster
